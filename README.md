# Assignment_3 — Sales Analysis Streamlit App with LangChain Agent

Diseñado y desarrollado por: 

- Eder José Hernández Buelvas
- Josué Pescador Ramos
- Juan Camilo Gallardo 

This repository contains a small data application that loads sales data and exposes a Streamlit UI and a LangChain-based agent for analytical queries and visual exports.

Contents
- `ui/streamlit_app.py` — Streamlit UI for interacting with the dataset and generated charts.
- `agent/` — Agent code (LangChain integration, query parsing and actions).
- `data/ventas.csv` — Example sales data used by the app.
- `db/init.sql` — SQL initialization script (create tables, sample data) if you want to initialize a Postgres DB.
- `exported/` — Output images and CSVs generated by the app/agent.
- `Dockerfile`, `docker-compose.yml` — Containerized setup for the application and a Postgres database.
- `requirements.txt` — Python dependencies used by the application.

Project overview

This project provides a Streamlit interface that visualizes sales data from `data/ventas.csv` and an agent (LangChain) that can run higher-level queries and actions (see `agent/`). The app can be run locally using Python or inside Docker using the provided `Dockerfile` and `docker-compose.yml`.

Quick start (Docker)

1. Build and start the services:

```bash
docker compose up --build
```

2. Open the Streamlit UI in your browser:

```text
http://localhost:8501
```

Notes:
- The `docker-compose.yml` starts two services: `db` (Postgres 16) and `app` (the Streamlit app). The compose file expects environment variables from a `.env` file (see Environment variables section).
- If you need only the Streamlit app locally and not Postgres, you can run the app without Docker (instructions below).

Quick start (local / virtualenv)

1. Create a virtual environment and install dependencies:

```bash
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

2. Run the Streamlit app:

```bash
streamlit run ui/streamlit_app.py --server.port=8501
```

Open `http://localhost:8501` in your browser.

Environment variables

The Docker compose references an `.env` file. Typical variables you may set:

- `POSTGRES_USER` — Postgres username (default in `docker-compose.yml`: `user`).
- `POSTGRES_PASSWORD` — Postgres password (default in `docker-compose.yml`: `password`).
- `POSTGRES_DB` — Postgres database name (default: `mydb`).
- `AWS_ACCESS_KEY_ID` — Amazon Web Services Acess Key 
- `AWS_SECRET_ACCESS_KEY` — Amazon Web Services Secret Key
- `AWS_SESSION_TOKEN` — Amazon Web Services Session Token
- `AWS_DEFAULT_REGION` — AWS Region in which the model is available
 
Create a `.env` file in the project root if you want to override defaults, for example:

```env
POSTGRES_USER=user
POSTGRES_PASSWORD=password
POSTGRES_DB=mydb
# any other env vars used by the app
```

Database initialization

The repository includes `db/init.sql` which defines schema and seed data. Ways to apply it:

- If you're running Postgres locally or have `psql` available on the host:

```bash
# requires psql client installed
psql "postgresql://user:password@localhost:5432/mydb" -f db/init.sql
```

- Alternatively, after you run `docker compose up -d db`, you can exec into the running container and apply the script (container name may vary):

```bash
docker compose exec db psql -U "$POSTGRES_USER" -d "$POSTGRES_DB" -f /var/lib/postgresql/data/init.sql || true
```

If the above path doesn't exist in the container, copy the file into the container or run `psql` from a client against the exposed port.

Using the app

- The Streamlit UI loads `data/ventas.csv` and provides visualizations and export options.
- The `agent/` folder contains:
  - `langchain_agent.py` — wiring for a LangChain agent that can parse and perform actions.
  - `query_parser.py` — a small parser for user natural language queries.
  - `actions.py` — action implementations (e.g., run queries, generate plots, export CSVs/images into `exported/`).

Outputs

- Generated charts and CSV outputs are placed into the `exported/` folder. Check there after running the app/agent.

Development notes

- Python path is set to `/app` in the Dockerfile using `ENV PYTHONPATH="/app"` so imports using `agent.*` and `ui.*` work in container mode.
- The Dockerfile installs system packages required for `psycopg2` and other packages.

Testing and quick checks

- There are no automated tests included in this repository by default. To quickly smoke-test the code after installing dependencies, run the Streamlit app and try the UI flows that load `data/ventas.csv` and trigger the agent actions. If you modify code in `agent/`, restart the Streamlit app to pick up changes.

Edge cases and considerations

1. Missing or malformed `data/ventas.csv`: the app should handle the file not existing or containing unexpected headers; if not, validate and fix the CSV before running.
2. Database not reachable: when running the agent or DB-backed features, ensure Postgres is running and accessible on `localhost:5432` (or the container network). Check `.env` and compose logs.
3. Large datasets / memory: Streamlit runs in-process; very large CSVs may cause high memory usage. Consider sampling or using pagination for large exports.
4. Concurrency: multiple users modifying `exported/` simultaneously could cause filename collisions. The agent names files with randomized tokens (see `exported/` content) — review and improve naming if needed.

Troubleshooting

- Streamlit page not reachable: confirm the app is running and listening on port `8501`. If using Docker, run `docker compose ps` and check logs with `docker compose logs app`.
- Database connection errors: check `docker compose logs db` for startup errors. Ensure the `POSTGRES_*` variables match between `.env` and the client.
- Dependency issues: if `pip install -r requirements.txt` fails, confirm you are on Python 3.10 and that system dependencies (`gcc`, `libpq-dev`) are installed (the Dockerfile installs them for container builds).

Next steps / Improvements

- Add automated unit tests for `agent/query_parser.py` and `agent/actions.py`.
- Add a simple integration test that starts the app (or a subset) and verifies an API endpoint or generated CSV exists.
- Add a `.env.example` to document the environment variables used by the project.

